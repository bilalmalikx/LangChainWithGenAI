{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ca1aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 1: Load the PDF\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "documents = loader.load()  # ✅ This returns a list of Document objects\n",
    "\n",
    "# Step 2: Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "final_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Step 3: Print or check\n",
    "print(final_docs[:2])  # Check first 2 chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee944e5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m      3\u001b[39m text_splitter=RecursiveCharacterTextSplitter(chunk_size=\u001b[32m500\u001b[39m,chunk_overlap=\u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m final_docs=text_splitter.split_documents(\u001b[43mloader1\u001b[49m)\n\u001b[32m      5\u001b[39m final_docs\n",
      "\u001b[31mNameError\u001b[39m: name 'loader1' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_docs=text_splitter.split_documents(loader1)\n",
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76a40bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x1130530e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b06ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='have been impossible. We shall, happily, still have an opportunity to prove that friendship in our'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "\n",
    "speech=\"\"\n",
    "\n",
    "with open('speech.txt') as f:\n",
    "    speech=f.read()\n",
    "    \n",
    "speech\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=10)\n",
    "text=text_splitter.create_documents([speech])\n",
    "print(text[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e51ab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 470, which is longer than the specified 100\n",
      "Created a chunk of size 347, which is longer than the specified 100\n",
      "Created a chunk of size 668, which is longer than the specified 100\n",
      "Created a chunk of size 982, which is longer than the specified 100\n",
      "Created a chunk of size 789, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.'\n",
      "page_content='Just because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the splitter\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "# Use create_documents because it's raw text\n",
    "docs = text_splitter.create_documents([speech])\n",
    "\n",
    "# Print a sample\n",
    "print(docs[0])\n",
    "print(docs[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf91fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
